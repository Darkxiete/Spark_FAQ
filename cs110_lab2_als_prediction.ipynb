{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测电影评级\n",
    "大数据最常见的用途之一是预测用户想要的内容。这样，Google就可以向您展示相关广告，亚马逊推荐相关产品，以及Netflix推荐您喜欢的电影。本实验将演示如何使用Apache Spark向用户推荐电影。我们将从一些基本技术开始，然后使用Spark ML库的Alternating Least Squares方法进行更复杂的预测。\n",
    "\n",
    "对于本实验，我们将使用2000万个评级的子集数据集。此数据集预先安装在Databricks上，来自MovieLens稳定基准评级数据集。但是，您编写的相同代码也适用于完整数据集（尽管使用Community Edition上的完整数据集运行可能需要相当长的时间）。\n",
    "\n",
    "在这个实验室中：\n",
    "- 第0部分：预赛\n",
    "- 第1部分：基本建议\n",
    "- 第2部分：协作过滤\n",
    "- 第3部分：对自己的预测\n",
    "\n",
    "正如在第一个学习Spark实验室中提到的那样，在对任何数据集调用collect（）之前要仔细考虑。当您使用一个小数据集时，调用collect（）然后使用Python来了解本地数据（在驱动程序中）将正常工作，但是当您使用不具有大数据集的大数据集时，这将不起作用适合一台机器的内存。调用collect（）并进行可能用Spark完成的本地分析的解决方案可能会在自动编程器中失败并且不会获得完全信用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码\n",
    "可以使用基本Python和pySpark DataFrame转换和操作完成此分配。 不需要数学以外的库。 除了我们在此作业中引入的ML函数之外，您应该只使用您在之前的实验练习中使用的Spark函数来完成此作业的所有部分（尽管如果您可以使用Spark的更多功能） 喜欢！）。\n",
    "\n",
    "我们将使用电影数据，去年使用的CS100.1x数据相同。 但是，在本课程中，我们使用的是DataFrame，而不是RDD。\n",
    "\n",
    "以下单元格定义数据文件的位置。 如果您想在自己的机器上运行本实验的导出版本（即在Databricks之外），您需要下载自己的2000万电影数据集副本，并且需要调整下面的路径。\n",
    "\n",
    "**待办事项**：运行以下单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from databricks_test_helper import Test\n",
    "\n",
    "dbfs_dir = '../ml-latest-small/'\n",
    "ratings_filename = os.path.join(dbfs_dir, 'ratings.csv')\n",
    "movies_filename = os.path.join(dbfs_dir, 'movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第0部分：预赛\n",
    "我们读入每个文件并创建一个由解析行组成的DataFrame。\n",
    "\n",
    "**2000万电影样本**\n",
    "2000万电影样本由CSV文件（带标题）组成，因此无需手动解析文件，因为Spark CSV可以完成这项工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dbutils\n",
    "# display(dbutils.fs.ls(dbfs_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU与I / O权衡\n",
    "请注意，我们有压缩文件（以.gz结尾）和未压缩文件。 我们这里有CPU与I / O权衡。 如果I / O是瓶颈，那么我们想要处理压缩文件并支付额外的CPU开销。 如果CPU是瓶颈，则处理未压缩文件更有意义。\n",
    "\n",
    "我们已经做了一些实验，我们已经确定在Community Edition上，CPU比I / O更容易成为瓶颈。 因此，我们将处理未压缩的数据。 此外，我们将通过显式指定DataFrame架构来进一步加快速度。 （当Spark CSV适配器从CSV文件中推断出架构时，它必须对文件进行额外的传递。这会减慢这里的速度，而且实际上并不是必需的。）\n",
    "\n",
    "**待办事项**：运行以下单元格，该单元格将定义模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "  [StructField('userId', IntegerType()),\n",
    "   StructField('movieId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('ID', IntegerType()),\n",
    "   StructField('title', StringType())]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master('local[*]') \\\n",
    "     .appName('Daddy') \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载和缓存\n",
    "Databricks文件系统（DBFS）位于S3之上。 我们将要访问这些数据。 我们不是一遍又一遍地从S3中读取它，而是将电影DataFrame和评级DataFrame缓存在内存中。\n",
    "\n",
    "**待办事项**：运行以下单元格以加载和缓存数据。 请耐心等待：代码大约需要30秒才能运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, title: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = spark.read.format('csv') \\\n",
    "                .options(header=True, inferSchema=False) \\\n",
    "                .schema(ratings_df_schema).load(ratings_filename)\n",
    "\n",
    "movies_df = spark.read.format('csv') \\\n",
    "                .options(header=True, inferSchema=False) \\\n",
    "                .schema(movies_df_schema).load(movies_filename)\n",
    "ratings_df.cache()\n",
    "movies_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第1部分：基本建议\n",
    "推荐电影的一种方法是始终推荐平均评分最高的电影。在这部分中，我们将使用Spark查找平均评分最高且至少500条评论的20部电影的名称，评分数和平均评分。我们希望过滤我们的电影评分较高但大于或等于500条评论，因为评论较少的电影可能不会对每个人产生广泛的吸引力。\n",
    "\n",
    "### （1a）平均评分最高的电影\n",
    "让我们确定平均评分最高的电影。\n",
    "\n",
    "您应该执行的步骤是：\n",
    "\n",
    "1. 回想一下ratings_df包含三列：\n",
    "- 评价电影的用户的ID\n",
    "- 被评级的电影的ID\n",
    "- 和评级。\n",
    "\n",
    "首先，将ratings_df转换为第二个DataFrame，movie_ids_with_avg_ratings，其中包含以下列：\n",
    "\n",
    "- 电影ID\n",
    "- 电影的评分数量\n",
    "- 所有电影评级的平均值\n",
    "\n",
    "2. 将movie_ids_with_avg_ratings转换为另一个DataFrame，movie_names_with_avg_ratings_df，将电影名称添加到每一行。 movie_names_with_avg_ratings_df将包含以下列：\n",
    "\n",
    "- 电影ID\n",
    "- 电影名称\n",
    "- 电影的评分数量\n",
    "- 所有电影评级的平均值\n",
    "\n",
    "**提示**：你需要加入。\n",
    "\n",
    "您最终应该得到以下内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ids_with_avg_ratings_df:\n",
      "+-------+-----+-----------------+\n",
      "|movieId|count|          average|\n",
      "+-------+-----+-----------------+\n",
      "|   1580|  165|3.487878787878788|\n",
      "|   2366|   25|             3.64|\n",
      "|   3175|   75|             3.58|\n",
      "+-------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "movie_names_with_avg_ratings_df:\n",
      "+-------+-----+-----------------+--------------------+\n",
      "|movieId|count|          average|               title|\n",
      "+-------+-----+-----------------+--------------------+\n",
      "|   1580|  165|3.487878787878788|Men in Black (a.k...|\n",
      "|   2366|   25|             3.64|    King Kong (1933)|\n",
      "|   3175|   75|             3.58| Galaxy Quest (1999)|\n",
      "+-------+-----+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "movie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId') \\\n",
    "        .agg(\n",
    "                F.count(ratings_df.rating).alias(\"count\"),\n",
    "                F.avg(ratings_df.rating).alias(\"average\"))\n",
    "\n",
    "print(\"movie_ids_with_avg_ratings_df:\")\n",
    "movie_ids_with_avg_ratings_df.show(3)\n",
    "\n",
    "movie_names_df = \\\n",
    "    movie_ids_with_avg_ratings_df \\\n",
    "        .join(movies_df, \n",
    "              movie_ids_with_avg_ratings_df['movieId']==movies_df['Id'])\n",
    "movie_names_with_avg_ratings_df = movie_names_df.drop('ID')\n",
    "print(\"movie_names_with_avg_ratings_df:\")\n",
    "movie_names_with_avg_ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1b）具有最高平均评分和至少250评论的电影\n",
    "现在我们有一个平均收视率最高的电影的数据框，我们可以使用Spark来确定平均收视率最高的20部电影和至少250个评论。\n",
    "\n",
    "添加单个DataFrame转换（代替下面的 \\\\<FILL_IN\\\\>），将结果限制为评分至少为250人的电影。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with highest ratings:\n",
      "+-------+-----+-----------------+-----------------------------------------+\n",
      "|movieId|count|average          |title                                    |\n",
      "+-------+-----+-----------------+-----------------------------------------+\n",
      "|296    |307  |4.197068403908795|Pulp Fiction (1994)                      |\n",
      "|593    |279  |4.161290322580645|Silence of the Lambs, The (1991)         |\n",
      "|2571   |278  |4.192446043165468|Matrix, The (1999)                       |\n",
      "|318    |317  |4.429022082018927|Shawshank Redemption, The (1994)         |\n",
      "|356    |329  |4.164133738601824|Forrest Gump (1994)                      |\n",
      "|260    |251  |4.231075697211155|Star Wars: Episode IV - A New Hope (1977)|\n",
      "+-------+-----+-----------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_with_250_ratings_or_more = \\\n",
    "    movie_names_with_avg_ratings_df.where(movie_names_with_avg_ratings_df[\"count\"]>=250)\n",
    "print('Movies with highest ratings:')\n",
    "movies_with_250_ratings_or_more.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用评论数量的阈值是改进建议的一种方法，但还有许多其他改善质量的好方法。 例如，您可以通过评级数量来评分。\n",
    "\n",
    "## 第2部分：协作过滤\n",
    "在本课程中，您了解了Spark允许我们应用于分布式数据集的许多基本转换和操作。 Spark还暴露了一些更高级别的功能; 特别是，机器学习使用Spark的一个组件称为MLlib。 在本部分中，您将学习如何使用MLlib使用我们一直在分析的电影数据制作个性化的电影推荐。\n",
    "我们将使用一种称为协同过滤的技术。协同过滤是一种通过从许多用户收集偏好或品味信息（协作）来自动预测（过滤）关于用户兴趣的方法。协同过滤方法的基本假设是，如果一个人A在一个问题上与一个人B具有相同的意见，那么A更有可能对另一个问题x有一个B的意见，而不是对一个人选择x的意见随机。您可以在此处详细了解协作过滤。\n",
    "\n",
    "右侧的图像（来自维基百科）显示了使用协同过滤预测用户评级的示例。首先，人们评价不同的项目（如视频，图像，游戏）。之后，系统正在预测用户对项目的评级，用户尚未评级。这些预测建立在其他用户的现有评级之上，这些用户与活跃用户具有相似的评级。例如，在下面的图像中，系统已做出预测，即活动用户不喜欢该视频。\n",
    "\n",
    "---\n",
    "\n",
    "对于电影推荐，我们从一个矩阵开始，其条目是用户的电影评级（在下图中以红色显示）。每列代表一个用户（以绿色显示），每行代表一个特定的电影（以蓝色显示）。\n",
    "\n",
    "由于并非所有用户都对所有电影进行了评分，因此我们不知道此矩阵中的所有条目，这正是我们需要协作过滤的原因。对于每个用户，我们只对部分电影进行评分。通过协同过滤，我们的想法是通过将其分解为两个矩阵的乘积来近似评级矩阵：一个描述每个用户的属性（以绿色显示），另一个描述每个电影的属性（以蓝色显示）。\n",
    "![图1](./pic/matrix_factorization.png)\n",
    "我们想要选择这两个矩阵，以便最小化我们知道正确评级的用户/电影对的错误。 [交替最小二乘算法(ASL)](https://en.wikiversity.org/wiki/Least-Squares_Method)通过首先用值随机填充用户矩阵然后优化电影的值使得误差最小化来实现这一点。 然后，它保持电影矩阵不变并优化用户矩阵的值。 这种在优化矩阵之间的交替是名称中“交替”的原因。\n",
    "\n",
    "此优化正如上图中右侧所示。 给定一组固定的用户因素（即用户矩阵中的值），我们使用已知的评级来使用在图的底部公式的优化来找到电影因子的最佳值。 然后我们“交替”并选择给定固定电影因素的最佳用户因素。\n",
    "\n",
    "有关用户和电影矩阵的简单示例，请查看[第2讲](https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/courseware/9d251397874d4f0b947b606c81ccf83c/3cf61a8718fe4ad5afcd8fb35ceabb6e/)的[视频或第8讲](https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/fb269ff9a53b669a46d59e154b876d78/asset-v1:BerkeleyX+CS110x+2T2016+type@asset+block/Lecture2s.pdf)的幻灯片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2a）创建训练集\n",
    "在我们开始使用机器学习之前，我们需要将ratings_df数据集分解为三个部分：\n",
    "\n",
    "- 训练集（DataFrame），我们将用它来训练模型\n",
    "- 验证集（DataFrame），我们将使用它来选择最佳模型\n",
    "- 测试集（DataFrame），我们将用于我们的实验\n",
    "\n",
    "要将数据集随机拆分为多个组，我们可以使用pySpark randomSplit（）转换。 randomSplit（）接受一组拆分和种子并返回多个DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 60343, validation: 20222, test: 20271\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      1|   4.0|\n",
      "|     1|      3|   4.0|\n",
      "|     1|     47|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      6|   4.0|\n",
      "|     1|    235|   4.0|\n",
      "|     1|    349|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    157|   5.0|\n",
      "|     1|    216|   5.0|\n",
      "|     1|    231|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_60_df, split_a_20_df, split_b_20_df = ratings_df.randomSplit([0.6,0.2,0.2],5)\n",
    "\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Training: {0}, validation: {1}, test: {2}\\n'.format(\n",
    "  training_df.count(), validation_df.count(), test_df.count())\n",
    ")\n",
    "training_df.show(3)\n",
    "validation_df.show(3)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆分数据集后，您的训练集大约有1200万个条目，验证和测试集各有大约400万个条目。 （由于randomSplit（）转换的随机性，每个数据集中的条目的确切数量会略有不同。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2b）交替最小二乘\n",
    "在这一部分中，我们将使用Apache Spark ML Pipeline实现交替最小二乘法，ALS。 ALS采用训练数据集（DataFrame）和几个控制模型创建过程的参数。要确定参数的最佳值，我们将使用ALS训练多个模型，然后我们将选择最佳模型，并在本练习的其余部分中使用该模型中的参数。\n",
    "\n",
    "我们将用于确定最佳模型的过程如下：\n",
    "\n",
    "1. 选择一组模型参数。模型最重要的参数是秩，即用户矩阵中的列数（上图中的绿色）或电影矩阵中的行数（上图中的蓝色）。通常，较低等级将意味着训练数据集上的较高误差，但是较高等级可能导致过度拟合。我们将使用training_df数据集训练排名为4,8和12的模型。\n",
    "\n",
    "2. 在ALS对象上设置适当的参数：\n",
    "\n",
    "- “用户”列将设置为userId DataFrame列中的值。\n",
    "- “Item”列将设置为movieId DataFrame列中的值。\n",
    "- “评级”列将设置为我们的评级DataFrame列中的值。\n",
    "- 我们将使用0.1的正则化参数。\n",
    "\n",
    "**注意**：请仔细阅读ALS类的文档。它将帮助您完成此步骤。\n",
    "\n",
    "3. 让ALS输出转换（即ALS.fit（）的结果）产生一个名为“预测”的新列，其中包含预测值。\n",
    "\n",
    "4. 使用ALS.fit（）创建多个模型，每个模型对应一个等级值。我们将适应训练数据集（training_df）。\n",
    "\n",
    "5. 对于每个模型，我们将针对我们的验证数据集（validation_df）运行预测并检查错误。\n",
    "\n",
    "6. 我们将保持模型的最佳错误率。\n",
    "\n",
    "### 我们为什么要进行自己的交叉验证？\n",
    "协作过滤的一个挑战是如何为新用户（根本没有提供任何评级的用户）提供评级。一些推荐系统选择向新用户提供一组默认评级（例如，所有评级的平均值），而其他推荐系统选择不为新用户提供评级。当要求为新用户提供评级时，Spark的ALS算法会生成NaN（非数字）值。\n",
    "\n",
    "因此，使用具有ALS的ML管道的CrossValidator是有问题的，因为交叉验证涉及将训练数据划分为一组折叠（例如，三组），然后使用这些折叠来在参数网格搜索过程期间测试和评估参数。某些折叠可能包含不在其他折叠中的用户，因此，ALS会为这些新用户生成NaN值。当CrossValidator使用Evaluator（RMSE）计算错误度量时，RMSE算法将返回NaN。这将使参数网格中的所有参数看起来同样好（或坏）。\n",
    "\n",
    "您可以阅读Spark JIRA 14489关于此问题的讨论。建议使用ALS提供默认值或使RMSE降低NaN值的变通方法。两者都引入潜在问题。我们选择让RMSE降低NaN值。虽然这不能解决ALS不能预测新用户价值的根本问题，但它确实提供了一些评估价值。我们使用for循环（下面）手动实现参数网格搜索过程，并在使用RMSE之前删除NaN值。\n",
    "\n",
    "对于生产应用程序，您可能需要考虑如何处理新用户的权衡。\n",
    "\n",
    "**注意**：此单元格可能需要几分钟才能运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.9192889621485235\n",
      "[ALS_44bbbfa2439c495e80a1, 0, 0]\n",
      "For rank 8 the RMSE is 0.9227985582444523\n",
      "[ALS_44bbbfa2439c495e80a1, ALS_44bbbfa2439c495e80a1, 0]\n",
      "For rank 12 the RMSE is 0.9148624839481305\n",
      "[ALS_44bbbfa2439c495e80a1, ALS_44bbbfa2439c495e80a1, ALS_44bbbfa2439c495e80a1]\n",
      "The best mode was trained with rank 12\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS()\n",
    "\n",
    "seed = 5\n",
    "\n",
    "als.setMaxIter(5) \\\n",
    "   .setSeed(seed) \\\n",
    "   .setRegParam(0.1) \\\n",
    "   .setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "reg_eval = RegressionEvaluator(\n",
    "                predictionCol=\"prediction\", \n",
    "                labelCol='rating', \n",
    "                metricName='rmse')\n",
    "\n",
    "tolerance = 0.03\n",
    "ranks = [4, 8, 12]\n",
    "# ranks = [12, 8, 4]\n",
    "errors = [0, 0, 0]\n",
    "models = [0, 0, 0]\n",
    "err = 0\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "for rank in ranks:\n",
    "    als.setRank(rank)\n",
    "    model = als.fit(training_df)\n",
    "    predict_df = model.transform(validation_df)\n",
    "    \n",
    "    predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "    \n",
    "    error = reg_eval.evaluate(predicted_ratings_df)\n",
    "    errors[err] = error\n",
    "    models[err] = model\n",
    "    print(\"For rank %s the RMSE is %s\" % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = err\n",
    "    err += 1\n",
    "    print(models)\n",
    "als.setRank(ranks[best_rank])\n",
    "print(\"The best mode was trained with rank %s\" % ranks[best_rank])\n",
    "my_model = models[best_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2c）测试您的模型\n",
    "到目前为止，我们使用training_df和validation_df数据集来选择最佳模型。 由于我们使用这两个数据集来确定哪种模型最好，我们不能用它们来测试模型的好坏; 否则，我们很容易过度拟合。 为了确定我们的模型有多好，我们需要使用test_df数据集。 我们将使用您在（2b）部分中确定的best_rank来创建用于预测测试数据集的评级的模型，然后我们将计算RMSE。\n",
    "\n",
    "您应该执行的步骤是：\n",
    "\n",
    "- 使用上面创建的my_model在测试数据集（test_df）上运行预测，生成新的predict_df DataFrame。\n",
    "- 过滤掉不需要的NaN值（由于Spark中的错误，这是必需的）。 我们已经为您提供了这段代码。\n",
    "- 使用先前创建的RMSE评估程序reg_eval来评估过滤的DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的误差为: 0.9167305919316049\n",
      "测试集上的数据预测示例\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|   409|    471|   3.0| 3.9891672|\n",
      "|   372|    471|   3.0| 3.0194802|\n",
      "|   182|    471|   4.5| 3.3084137|\n",
      "|   474|    471|   3.0| 2.9631383|\n",
      "|   273|    471|   5.0| 4.6632466|\n",
      "|   411|    471|   4.0| 3.1806765|\n",
      "|   260|    471|   4.5| 3.5837674|\n",
      "|   373|    471|   5.0| 3.5970507|\n",
      "|   606|   1088|   3.0|  3.119496|\n",
      "|   132|   1088|   4.0|  3.202322|\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_df = my_model.transform(test_df)\n",
    "\n",
    "predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "\n",
    "error = reg_eval.evaluate(predicted_ratings_df)\n",
    "\n",
    "print(\"测试集上的误差为: %s\" % error)\n",
    "print(\"测试集上的数据预测示例\")\n",
    "predicted_ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2d）比较你的模型\n",
    "查看模型预测的结果与测试集中的值的RMSE是评估模型质量的一种方法。 评估模型的另一种方法是评估测试集中的误差，其中每个评级是训练集的平均评级。\n",
    "\n",
    "您应该执行的步骤是：\n",
    "\n",
    "- 使用training_df计算该训练数据集中所有电影的平均评分。\n",
    "- 使用您刚刚确定的平均评分和test_df创建一个DataFrame（test_for_avg_df），其中包含一个包含平均评分的预测列。 提示：您将要使用来自`pyspark.sql.functions`的`lit()`函数，此处可用作`F.lit()`。\n",
    "- 使用我们先前创建的`reg_eval`对象来评估`test_for_avg_df`并计算RMSE。\n",
    "\n",
    "   \n",
    "\n",
    "```\n",
    "Help on function lit in module pyspark.sql.functions:\n",
    "\n",
    "lit(col)\n",
    "    Creates a :class:`Column` of literal value.\n",
    "\n",
    "    >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
    "    [Row(height=5, spark_user=True)]\n",
    "\n",
    "    .. versionadded:: 1.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3部分：对自己的预测\n",
    "本练习的最终目标是预测要向自己推荐的电影。 为此，您首先需要为ratings_df数据集添加自己的评级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3a）你的电影评级\n",
    "\n",
    "为了帮助您为自己提供评分，我们提供了以下代码，列出了我们在实验室第1部分中创建的movies_with_500_ratings_or_more中评价最高的50部电影的名称和电影ID。\n",
    "\n",
    "movieId\tcount\taverage\ttitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户ID 0未分配，因此我们将其用于您的评级。 我们将变量my_user_ID设置为0。 接下来，创建一个名为my_ratings_df的新DataFrame，其评分至少为10个电影评级。 每个条目的格式应为（my_user_id，movieID，rating）。 与原始数据集一样，评级应介于1和5之间（含）。 如果你还没有看过这些电影中的至少10部，你可以在上面的单元格中增加传递给take（）的参数，直到你看过10部电影（或者你也可以猜出你对电影的评分是多少） 没有见过）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     0|   1193|   3.5|\n",
      "|     0|    914|   2.5|\n",
      "|     0|   2355|   4.2|\n",
      "|     0|   1287|   3.7|\n",
      "|     0|    594|   3.1|\n",
      "|     0|    595|   2.6|\n",
      "|     0|   2398|   1.7|\n",
      "|     0|   1035|   4.0|\n",
      "|     0|   2687|   5.0|\n",
      "|     0|   3105|   4.7|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "my_user_id = 0\n",
    "\n",
    "# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.\n",
    "my_rated_movies = [\n",
    "      (my_user_id, 1193, 3.5),\n",
    "      (my_user_id, 914, 2.5),\n",
    "      (my_user_id, 2355, 4.2),\n",
    "      (my_user_id, 1287, 3.7),\n",
    "      (my_user_id, 594, 3.1),\n",
    "      (my_user_id, 595, 2.6),\n",
    "      (my_user_id, 2398, 1.7),\n",
    "      (my_user_id, 1035, 4.0),\n",
    "      (my_user_id, 2687, 5.0),\n",
    "      (my_user_id, 3105, 4.7),\n",
    "      (my_user_id, 1270, 2.5),\n",
    "     # The format of each line is (my_user_id, movie ID, your rating)\n",
    "     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n",
    "     #   (my_user_id, 260, 5),\n",
    "]\n",
    "\n",
    "my_ratings_df = spark.createDataFrame(my_rated_movies, ['userId','movieId','rating'])\n",
    "print('My movie ratings:')\n",
    "display(my_ratings_df.show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3b）将您的电影添加到训练数据集\n",
    "现在您已经为自己获得了评分，您需要将您的评分添加到训练数据集中，以便您训练的模型将包含您的偏好。 Spark的unionAll（）转换结合了两个DataFrame; 使用unionAll（）创建一个新的训练数据集，其中包括您的评级和原始训练数据集中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset now has 11 more entries than the original training dataset\n"
     ]
    }
   ],
   "source": [
    "training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n",
    "\n",
    "print ('The training dataset now has %s more entries than the original training dataset' %\n",
    "       (training_with_my_ratings_df.count() - training_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3c）用您的评级训练模型\n",
    "现在，训练一个模型，其中添加了您的评级以及您在第（2b）和（2c）部分中使用的参数。 确保包含所有参数。\n",
    "\n",
    "注意：此单元格大约需要30秒才能运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "als.setPredictionCol('prediction') \\\n",
    "   .setMaxIter(5) \\\n",
    "   .setSeed(seed) \\\n",
    "   .setRegParam(0.1) \\\n",
    "   .setUserCol('userId').setItemCol('movieId').setRatingCol('rating') \\\n",
    "   .setRank(ranks[best_rank])\n",
    "\n",
    "my_ratings_model = als.fit(training_with_my_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3d）使用您的评级检查RMSE的新模型\n",
    "在测试集上计算此新模型的RMSE。\n",
    "\n",
    "根据test_df中设置的测试数据运行模型（刚训练的模型）。\n",
    "然后，使用我们先前计算的reg_eval对象来计算您的评级的RMSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.9219973035931176\n"
     ]
    }
   ],
   "source": [
    "my_predict_df = my_ratings_model.transform(test_df).cache()\n",
    "\n",
    "predicted_test_my_ratings_df = \\\n",
    "    my_predict_df.filter(my_predict_df.prediction != float('nan'))\n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中共有 0.047506289773568154 的数据无法预测\n"
     ]
    }
   ],
   "source": [
    "# 有一些值预测不出来？？\n",
    "# TODO\n",
    "nan_perc = my_predict_df.filter(my_predict_df.prediction == float('nan')).count() / \\\n",
    "    my_predict_df.count()\n",
    "print(\"测试集中共有 %s 的数据无法预测\" % nan_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'userId'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predict_df.filter(my_predict_df.prediction == float('nan'))['userId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3e）预测你的评分\n",
    "到目前为止，我们只计算了模型的误差。接下来，让我们预测您将给予您尚未提供评级的电影的评分。\n",
    "\n",
    "您应该执行的步骤是：\n",
    "\n",
    "- 过滤掉您已手动评分的电影。 （使用my_rated_movie_ids变量。）将结果放入新的not_rated_df中。\n",
    "\n",
    "**提示**：`Column.isin（）`方法以及`〜`（“not”）DataFrame逻辑运算符可能会派上用场。以下是使用`isin（）`的示例：\n",
    "```\n",
    "    > df1 = sqlContext.createDataFrame（[（\"Jim\",10），（“Julie”，9），（“Abdul”，20），（“Mireille”，19）]，[“name”，“age”] ）\n",
    "    > df1.show（）\n",
    "    + -------- + --- +\n",
    "    |名称     |年龄|\n",
    "    + -------- + --- +\n",
    "    |吉姆     | 10 |\n",
    "    |朱莉     | 9  |\n",
    "    |阿卜杜勒   | 20 |\n",
    "    |米雷耶    | 19 |\n",
    "    + -------- + --- +\n",
    "\n",
    "    > names_to_delete = [“Julie”，“Abdul”]＃这只是一个Python列表\n",
    "    > df2 = df1.filter（~df1 [“name”].isin（names_to_delete））＃“NOT IN”\n",
    "    > df2.show（）\n",
    "    + -------- + --- +\n",
    "    |名称     |年龄|\n",
    "    + -------- + --- +\n",
    "    |吉姆     | 10 |\n",
    "    |米雷耶    | 19 |\n",
    "    + -------- + --- +\n",
    "```\n",
    "- 通过以下方式将 `not_rated_df`转换为`my_unrated_movies_df`：\n",
    "    - 将“ID”列重命名为“movieId”\n",
    "    - 添加“userId”列，其中包含上面定义的my_user_id变量中包含的值。\n",
    "- 通过将`my_ratings_model`应用于`my_unrated_movies_df`来创建`predict_ratings_df DataFrame`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code\n",
    "\n",
    "# Create a list of my rated movie IDs\n",
    "my_rated_movie_ids = [x[1] for x in my_rated_movies]\n",
    "\n",
    "# Filter out the movies I already rated.\n",
    "not_rated_df = movies_df.filter(~ movies_df[\"ID\"].isin(my_rated_movie_ids))\n",
    "\n",
    "# Rename the \"ID\" column to be \"movieId\", and add a column with my_user_id as \"userId\".\n",
    "my_unrated_movies_df = not_rated_df.selectExpr(\"ID as movieId\").withColumn('userId', F.lit(my_user_id))\n",
    "\n",
    "# Use my_rating_model to predict ratings for the movies that I did not manually rate.\n",
    "raw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n",
    "\n",
    "predicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for LEFT OUTER join between logical plans\\nProject [ID#6 AS movieId#3794]\\n+- Filter NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687)\\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\\nand\\nProject [_2#3777 AS features#3780]\\n+- Filter (UDF(0) = _1#3776)\\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\\n      +- ExternalRDD [obj#3775]\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2319.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for LEFT OUTER join between logical plans\nProject [ID#6 AS movieId#3794]\n+- Filter NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687)\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\nand\nProject [_2#3777 AS features#3780]\n+- Filter (UDF(0) = _1#3776)\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\n      +- ExternalRDD [obj#3775]\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1125)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1122)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1122)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1104)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-1f3ed30daf36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_predicted_ratings_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for LEFT OUTER join between logical plans\\nProject [ID#6 AS movieId#3794]\\n+- Filter NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687)\\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\\nand\\nProject [_2#3777 AS features#3780]\\n+- Filter (UDF(0) = _1#3776)\\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\\n      +- ExternalRDD [obj#3775]\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "raw_predicted_ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected implicit cartesian product for LEFT OUTER join between logical plans\n",
      "Project [ID#6 AS movieId#3574]\n",
      "+- Filter NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687)\n",
      "   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\n",
      "and\n",
      "Project [_2#2883 AS features#2886]\n",
      "+- Filter (UDF(0) = _1#2882)\n",
      "   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#2882, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#2883]\n",
      "      +- ExternalRDD [obj#2881]\n",
      "Join condition is missing or trivial.\n",
      "Either: use the CROSS JOIN syntax to allow cartesian products between these\n",
      "relations, or: enable implicit cartesian products by setting the configuration\n",
      "variable spark.sql.crossJoin.enabled=true;\n"
     ]
    }
   ],
   "source": [
    "print('Detected implicit cartesian product for LEFT OUTER join between logical plans\\nProject [ID#6 AS movieId#3574]\\n+- Filter NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687)\\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\\nand\\nProject [_2#2883 AS features#2886]\\n+- Filter (UDF(0) = _1#2882)\\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#2882, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#2883]\\n      +- ExternalRDD [obj#2881]\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3f）预测您的评分\n",
    "我们有预测的评级。 现在我们可以打印出预测评分最高的25部电影。\n",
    "\n",
    "您应该执行的步骤是：\n",
    "\n",
    "使用`movie_names_with_avg_ratings_df DataFrame`加入您的`predict_ratings_df DataFrame`，以获取每部电影的评分计数。\n",
    "按预测评级（最高评级优先）对结果`DataFrame(predict_with_counts_df)`进行排序，并删除计数为75或更少的任何评级。\n",
    "打印剩下的前25部电影。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 25 highest rated movies as predicted (for movies with more than 75 reviews):\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for LEFT OUTER join between logical plans\\nProject [ID#6 AS movieId#3794]\\n+- Filter (NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687) && isnotnull(ID#6))\\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\\nand\\nProject [_2#3777 AS features#3780]\\n+- Filter (UDF(0) = _1#3776)\\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\\n      +- ExternalRDD [obj#3775]\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2399.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for LEFT OUTER join between logical plans\nProject [ID#6 AS movieId#3794]\n+- Filter (NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687) && isnotnull(ID#6))\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\nand\nProject [_2#3777 AS features#3780]\n+- Filter (UDF(0) = _1#3776)\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\n      +- ExternalRDD [obj#3775]\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1125)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1122)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1122)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1104)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor154.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-2ef55de0d4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'My 25 highest rated movies as predicted (for movies with more than 75 reviews):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredicted_highest_rated_movies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for LEFT OUTER join between logical plans\\nProject [ID#6 AS movieId#3794]\\n+- Filter (NOT ID#6 INSET (1287,3105,1270,2355,1035,2398,594,595,914,1193,2687) && isnotnull(ID#6))\\n   +- InMemoryRelation [ID#6, title#7], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) FileScan csv [ID#6,title#7] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/Dk/Spark/tutorial/ml-latest-small/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,title:string>\\nand\\nProject [_2#3777 AS features#3780]\\n+- Filter (UDF(0) = _1#3776)\\n   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#3776, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#3777]\\n      +- ExternalRDD [obj#3775]\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "predicted_with_counts_df = predicted_ratings_df \\\n",
    "            .join(movie_names_with_avg_ratings_df, \\\n",
    "                  movie_names_with_avg_ratings_df[\"movieId\"]==predicted_ratings_df[\"movieId\"])\n",
    "predicted_highest_rated_movies_df = predicted_with_counts_df \\\n",
    "            .filter(predicted_with_counts_df[\"count\"]>75) \\\n",
    "            .sort(\"prediction\",ascending=False)\n",
    "\n",
    "print ('My 25 highest rated movies as predicted (for movies with more than 75 reviews):')\n",
    "predicted_highest_rated_movies_df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
